{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37b6829",
   "metadata": {},
   "source": [
    "# Assignment Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6246b",
   "metadata": {},
   "source": [
    "## Programming Assignment (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dfa2a",
   "metadata": {},
   "source": [
    "The programming assignement will be an implementation of the task described in the assignment\n",
    "\n",
    "We will make sure you have enough scaffolding to build the code upon where you would only have to implement the interesting parts of the code\n",
    "\n",
    "### Evaluation\n",
    "The evaluation of the assignment will be done through test scripts that you would need to pass to get the points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71455cfd",
   "metadata": {},
   "source": [
    "## Written Assignment (60 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05348ced",
   "metadata": {},
   "source": [
    "Written assignment tests the understanding of the student for the assignment's task. We have split the writing into sections. You will need to write 1-2 paragraphs describing the sections. Please be concise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99841650",
   "metadata": {},
   "source": [
    "### In your own words, describe what the task is (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049c34a",
   "metadata": {},
   "source": [
    "#### HMM\n",
    " In this task I had to make an HMM model for Part of Speech tagging in the Hindi Language. Here I mostly worked on the fit and decode function to populate and understand the viterbii table.\n",
    "    \n",
    "#### CRF\n",
    " In this task I had to create a Conditional Random Field (CRF) model for Named Entity Recognition (NER) in the hindi language. We had to implement it using the POS obtained the first part as features. I mainly had to work on the feature recogntion, the training loop and finding the optimal hyper-parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cc70a",
   "metadata": {},
   "source": [
    "### Describe your method for the task (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb5ed1",
   "metadata": {},
   "source": [
    "Important details about the implementation. Feature engineering, parameter choice etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361966f-deb9-4df9-93a2-77388c606ae7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### HMM\n",
    "\n",
    "* **Fit**: For fit I mainly had to follow the instructions and fill the Pi, A and B tables. I iterated through each of the initial states,  state transition, and the state-observation emmision using the zip function to fill the specific tables.\n",
    "    \n",
    "* **Decode**: To decode I have created a viterbi tabel and initialise it using the pi and B table. After this I iterate through each subsequent word and fill the viterbi table by finding the max of the product of the previous path and the probability of the path. Then I multiply the maximum with the probility of the word being the given observation. Finally I set the backpointer so I have a path for how I might have reached the specific place. Once the table is complete I follow the backpointer to obtain the best path.\n",
    "    \n",
    "#### CRF\n",
    "* **Feature Recognition**: In this function we create multiple features for the model, the include the word itself, word length, previous/next word as well as the pos of the word and the words behing/after the word. We run the pos taagger on the entire sentence and pass it to the make_features function to assign the pos instead of claculating the pos of indivisual words.\n",
    "    \n",
    "* **Training Loop**: For the training loop we start by dividing the data into batches the we loop over all the batches calling a forward pass on the model and take the negative of that. This is followed by a backpropogation and an update of the weights. Finally the loss is appended in the loss array. Once all the batches have been completed we calculate the average loss over the batches and test the model on the dev set. We then calculate and print the F1 score for that epoch and move on to the next epoch.\n",
    "      \n",
    "* **Hyperparameters**: Here I have increased the batch size to 256 and decreased the learning rate to 0.025 so that the model trains faster. I ran the model for 40 epochs, however it seeemed to have stabilised near the 25th epoch and while the loss did decrease the F1 score only showed minor changes.\n",
    "\n",
    "* **Pickle**: For this assignment, while not neccesary I pickled the featurised data so that I did not need to run the program again after changing hmm.py or crf.py. This allowed me to store the featurized data, however that data is not visible on github as the files were too big to upload without Github LFS.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa84f3f",
   "metadata": {},
   "source": [
    "### Experiment Results (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa642620",
   "metadata": {},
   "source": [
    "Typically a table summarizing all the different experiment results for various parameter choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78477192-7b7c-4080-852c-751e8d3681ee",
   "metadata": {},
   "source": [
    "#### HMM\n",
    "For HMM I obtained an accuracy of 81.2% on the dev set and an accuracy of 79.87% on the test set. Since the difference is not so large we can assume the model works quite well and is not overfitting.\n",
    "| Dataset      | Accuracy |\n",
    "| -------------- | -------------- |\n",
    "| Dev      | 0.8127       |\n",
    "| Test   | 0.7987        |\n",
    "#### CRF\n",
    "For the CRF I obtained an F1 score of 0.973 on the dev set. The model trained quite well over the 40 epochs and I have shown a summary of the training bellow.\n",
    "| Epoch      | Loss | F1 Score |\n",
    "| -------------- | -------------- |--------------|\n",
    "| 5| 4232|0.93|\n",
    "| 10| 2455|0.95|\n",
    "| 15| 1946|0.96|\n",
    "| 20| 1458|0.96|\n",
    "| 25| 1351|0.95|\n",
    "| 30| 1309|0.97|\n",
    "| 35| 1148|0.97|\n",
    "| 40| 1075|0.97|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf3384",
   "metadata": {},
   "source": [
    "### Discussion (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f974b3",
   "metadata": {},
   "source": [
    "Key takeaway from the assignment. Why is the method good? shortcomings? how would you improve? Additional thoughts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba409eb-ae9d-460a-bff2-8989acbf426d",
   "metadata": {},
   "source": [
    "#### HMM\n",
    "The HMM worked quite well in identifying the part of speech. The method allows for us to look at the words that occure before it to more accurately predict the part of speech. One major weakness of the hidden markov model though is the indipendance assumption, this may cause the model to fumble in complex sentences. We could increase the degrees in the HMM to counteract this but some artefacts would still persist.\n",
    "\n",
    "#### CRF\n",
    "The CRF worked very well for named entity recognition, obtaining an F1 score of 0.97. The model loooks for fowards and backwards as well as taking the POS of the word. This POS allows for the model to be much more accurate, however it also a shortcoming as the POS prediction model is only 80% accurate so the feature given to the model are faulty as well. However since the accuracy doe not seem to suffer much we can assume the models moves around that error and provides the result based on other factors as well. The best way to improve the model would be to use word embeddings, or improving the accuracy of POS tagger. However despite everything the model has a good enough accuracy to not need more features for this purpose at least."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
